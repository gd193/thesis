\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{vi}{dummy.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{viii}{dummy.4}\protected@file@percent }
\citation{dilao2023dynamical}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Dynamical Systems}{1}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction to Dynamical Systems}{1}{section.7}\protected@file@percent }
\newlabel{def:dyn_sys}{{1.1}{1}{}{Item.11}{}}
\citation{strogatz2018nonlinear}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Continuous Dynamical Systems}{2}{subsection.12}\protected@file@percent }
\newlabel{eq:diff_eq_gen}{{1.1}{2}{Continuous Dynamical Systems}{equation.13}{}}
\newlabel{eq:periodic}{{1.5}{3}{Orbits}{equation.18}{}}
\newlabel{theo:exis_ode_local}{{1.7}{4}{Local existance and uniqueness of solutions of ordinary differential equations}{theorem.29}{}}
\newlabel{theo:exis_ode_global}{{1.8}{4}{Global existance and uniqueness of solutions of ordinary differential equations}{theorem.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Stability of Fixed points}{5}{subsection.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Difference Equations as Dynamical Systems}{6}{subsection.41}\protected@file@percent }
\newlabel{eq:discr_de_gen}{{1.10}{6}{Difference Equations as Dynamical Systems}{equation.42}{}}
\newlabel{eq:dicr_fp}{{1.11}{6}{}{equation.44}{}}
\citation{dilao2023dynamical}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Lyapunov Exponents}{8}{section.50}\protected@file@percent }
\newlabel{sec:lya_exp}{{1.2}{8}{Lyapunov Exponents}{section.50}{}}
\newlabel{eq:idea_le}{{1.16}{8}{Lyapunov Exponents}{equation.53}{}}
\newlabel{eq:jacobian_perturb}{{1.21}{9}{Lyapunov Exponents}{equation.58}{}}
\citation{benettin1980lyapunov}
\citation{vogt2022lyapunov}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Recurrent Neural Networks}{12}{chapter.65}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction to Recurrent Neural Networks}{12}{section.66}\protected@file@percent }
\newlabel{eq:vanilla_rnn}{{2.1}{12}{Introduction to Recurrent Neural Networks}{equation.67}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Unfolding the Computational Graph}{13}{section.68}\protected@file@percent }
\newlabel{eq:vanilla_dyn_sys}{{2.2}{13}{Unfolding the Computational Graph}{equation.69}{}}
\citation{chung2021turing}
\citation{schafer2006recurrent}
\citation{blum1988training}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Basic computational graph}}{14}{figure.caption.73}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:basic_graph}{{2.1}{14}{Basic computational graph}{figure.caption.73}{}}
\newlabel{eq:gen_loss}{{2.7}{14}{Unfolding the Computational Graph}{equation.75}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Backpropagation through time}{14}{section.77}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Full graph of a recurrent neural network}}{15}{figure.caption.76}\protected@file@percent }
\newlabel{fig:full_graph}{{2.2}{15}{Full graph of a recurrent neural network}{figure.caption.76}{}}
\newlabel{eq:bptt_grad}{{2.8}{15}{Backpropagation through time}{equation.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Graph representation of BPTT}}{16}{figure.caption.80}\protected@file@percent }
\newlabel{fig:bptt_graph}{{2.3}{16}{Graph representation of BPTT}{figure.caption.80}{}}
\newlabel{eq:Jacobian_product}{{2.9}{16}{Backpropagation through time}{equation.79}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Exploding and vanishing gradients}{16}{section.81}\protected@file@percent }
\newlabel{sec:evg_problem}{{2.4}{16}{Exploding and vanishing gradients}{section.81}{}}
\citation{bengio1993problem}
\citation{bengio1994learning}
\newlabel{eq:simple_rnn}{{2.10}{17}{Exploding and vanishing gradients}{equation.82}{}}
\newlabel{eq:bptt_evg}{{2.11}{17}{Exploding and vanishing gradients}{equation.83}{}}
\newlabel{eq:diag_power}{{2.12}{17}{Exploding and vanishing gradients}{equation.84}{}}
\citation{pascanu2013difficulty}
\citation{hochreiter1997long}
\citation{pascanu2013difficulty}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Dealing with the exploding and vanishing gradient}{18}{section.87}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}$L_1$ and $L_2$ Regularization}{18}{subsection.88}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Alternative architectures}{18}{subsection.89}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Gradient Clipping}{18}{subsection.90}\protected@file@percent }
\citation{le2015simple}
\citation{talathi2015improving}
\citation{williams1989learning}
\citation{jordan1986attractor}
\citation{pineda1988dynamics}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Pseudocode for gradient clipping\relax }}{19}{algorithm.91}\protected@file@percent }
\newlabel{alg:cap}{{1}{19}{Pseudocode for gradient clipping\relax }{algorithm.91}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Smart Initialization}{19}{subsection.92}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Teacher Forcing}{19}{section.93}\protected@file@percent }
\citation{williams1989learning}
\citation{NEURIPS2022_495e55f3}
\citation{doya1992bifurcations}
\newlabel{eq:teacher_forcing}{{2.15}{20}{Teacher Forcing}{equation.94}{}}
\citation{doya1992bifurcations}
\citation{durstewitz2017state}
\newlabel{eq:weak_tf}{{2.18}{21}{Teacher Forcing}{equation.97}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Piecewise Linear Recurrent Neural Network (PLRNN)}{21}{section.99}\protected@file@percent }
\newlabel{eq:plrnn}{{2.20}{21}{Piecewise Linear Recurrent Neural Network (PLRNN)}{equation.100}{}}
\citation{koppe2019identifying}
\citation{schmidt2019identifying}
\citation{brenner2022tractable}
\citation{mikhaeil2022difficulty}
\citation{brenner2022tractable}
\citation{hess2023generalized}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}PLRNN extensions}{22}{section.102}\protected@file@percent }
\newlabel{eq:dendPLRNN}{{2.22}{22}{PLRNN extensions}{equation.103}{}}
\newlabel{eq:shPLRNN}{{2.23}{22}{PLRNN extensions}{equation.104}{}}
\newlabel{eq:clippedShPLRNN}{{2.24}{23}{PLRNN extensions}{equation.105}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Linear Observation model}{23}{section.106}\protected@file@percent }
\newlabel{eq:obs_eq_basic}{{2.25}{23}{Linear Observation model}{equation.107}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Identity Teacher Forcing}{23}{section.108}\protected@file@percent }
\citation{mikhaeil2022difficulty}
\citation{mikhaeil2022difficulty}
\newlabel{eq:ident_obs}{{2.28}{24}{Identity Teacher Forcing}{equation.111}{}}
\newlabel{eq:indent_tf}{{2.29}{24}{Identity Teacher Forcing}{equation.112}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.11}Inversion Teacher Forcing}{24}{section.113}\protected@file@percent }
\newlabel{eq:inversion_tf}{{2.30}{24}{Inversion Teacher Forcing}{equation.114}{}}
\citation{fox2012coupling}
\citation{penny2011statistical}
\@writefile{toc}{\contentsline {section}{\numberline {2.12}BOLD observation model}{25}{section.116}\protected@file@percent }
\newlabel{sec:bold_obs}{{2.12}{25}{BOLD observation model}{section.116}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.1}Physical principals of fMRI}{25}{subsection.117}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.2}Modelling the Hemodynamic Response}{25}{subsection.118}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Comparison of $hrf$ functions with varying repetition times}}{26}{figure.caption.120}\protected@file@percent }
\newlabel{fig:hrf_comp}{{2.4}{26}{Comparison of $hrf$ functions with varying repetition times}{figure.caption.120}{}}
\newlabel{eq:canonical_hrf}{{2.32}{26}{Modelling the Hemodynamic Response}{equation.119}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12.3}Defining the BOLD observation equation}{27}{subsection.122}\protected@file@percent }
\newlabel{sec:bold_obs_eq}{{2.12.3}{27}{Defining the BOLD observation equation}{subsection.122}{}}
\newlabel{eq:bold_obs_eq}{{2.34}{27}{Defining the BOLD observation equation}{equation.123}{}}
\citation{serov2017fourier}
\citation{smith1997scientist}
\citation{puthusserypady2021applied}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Fourier Transform}{28}{chapter.124}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Continuous Fourier Transform}{28}{section.125}\protected@file@percent }
\newlabel{def:fourier_trafo}{{3.1}{28}{Continuous Fourier Transform}{equation.127}{}}
\newlabel{prop:fourier_properties}{{3.3}{29}{Properties of the Fourier transform}{Item.134}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Discrete Fourier Transform}{29}{section.135}\protected@file@percent }
\newlabel{eq:dft}{{3.4}{29}{Discrete Fourier Transform}{equation.137}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Unitary DFT}{30}{subsection.140}\protected@file@percent }
\newlabel{sec:unitary_dft}{{3.2.1}{30}{Unitary DFT}{subsection.140}{}}
\newlabel{eq:dft_matrix}{{3.4}{30}{Unitary DFT}{equation.141}{}}
\newlabel{th:plancherel}{{3.6}{31}{Plancherel theorem}{equation.144}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Fast Fourier Transform}{31}{section.145}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Cooley–Tukey FFT algorithm}{31}{subsection.146}\protected@file@percent }
\newlabel{eq:fft_double_sum}{{3.7}{31}{Cooley–Tukey FFT algorithm}{equation.147}{}}
\newlabel{eq:n_composite_number}{{3.9}{32}{Cooley–Tukey FFT algorithm}{equation.149}{}}
\citation{FFTW.jl-2005}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Basic butterfly diagram in the decimation-in-time FFT algorithm\relax }}{33}{figure.caption.154}\protected@file@percent }
\newlabel{fig:butterfly_basic}{{3.1}{33}{Basic butterfly diagram in the decimation-in-time FFT algorithm\relax }{figure.caption.154}{}}
\newlabel{eq:fft_radix_decomp}{{3.12}{33}{Cooley–Tukey FFT algorithm}{equation.152}{}}
\newlabel{eq:fft_butterfly}{{3.13}{33}{Cooley–Tukey FFT algorithm}{equation.153}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Visualization of a 3 stage radix-2 FFT algorithm\relax }}{34}{figure.caption.155}\protected@file@percent }
\newlabel{fig:butterfly_fft}{{3.2}{34}{Visualization of a 3 stage radix-2 FFT algorithm\relax }{figure.caption.155}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Convolution and the Convolutional Theorem}{34}{section.156}\protected@file@percent }
\newlabel{sec:conv}{{3.4}{34}{Convolution and the Convolutional Theorem}{section.156}{}}
\newlabel{eq:conv_def}{{3.7}{34}{Convolution}{equation.158}{}}
\newlabel{eq:disc_conv}{{3.15}{34}{Discrete Convolution}{equation.160}{}}
\newlabel{eq:disc_conv_finite}{{3.16}{34}{Discrete Convolution}{equation.161}{}}
\newlabel{eq:conv_matrix}{{3.17}{35}{Convolution and the Convolutional Theorem}{equation.162}{}}
\newlabel{prop:conv_properties}{{3.9}{36}{Properties of the Convolution operation}{Item.168}{}}
\newlabel{th:conv_theorem}{{3.10}{36}{Convolutional Theorem}{equation.171}{}}
\citation{jones1970problem}
\citation{wiener1964extrapolation}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Deconvolution}{37}{section.172}\protected@file@percent }
\newlabel{eq:conv_eq}{{3.20}{37}{Deconvolution}{equation.173}{}}
\newlabel{eq:inverse_filtering}{{3.23}{37}{Deconvolution}{equation.176}{}}
\newlabel{eq:conv_real}{{3.24}{37}{Deconvolution}{equation.177}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Wiener Deconvolution}{37}{subsection.178}\protected@file@percent }
\newlabel{sec:Wiener_deconv}{{3.5.1}{37}{Wiener Deconvolution}{subsection.178}{}}
\newlabel{eq:wiener_deconv}{{3.26}{38}{Wiener Deconvolution}{equation.180}{}}
\newlabel{eq:wiener_filter_fourier}{{3.27}{38}{Wiener Deconvolution}{equation.181}{}}
\citation{sauer-notes}
\citation{mallat2008wavelet}
\citation{shah2022wavelet}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Wavelet Transformation}{39}{chapter.183}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Continuous Wavelet Transformation}{39}{section.184}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Schematic overview of time/frequency resolution of different transformations}}{40}{figure.caption.185}\protected@file@percent }
\newlabel{fig:wavelet_trafo_sketch}{{4.1}{40}{Schematic overview of time/frequency resolution of different transformations}{figure.caption.185}{}}
\newlabel{def:wavelet}{{4.1}{41}{Wavelet}{Item.189}{}}
\newlabel{eq:def_cwt}{{4.4}{41}{Continuous wavelet transform}{equation.194}{}}
\newlabel{def:cwt}{{4.2}{41}{Continuous wavelet transform}{equation.194}{}}
\newlabel{eq:cwt_tayler_exp}{{4.5}{42}{Continuous Wavelet Transformation}{equation.208}{}}
\newlabel{eq:cwt_with_ft}{{4.6}{43}{Continuous Wavelet Transformation}{equation.210}{}}
\newlabel{eq:ft_of_cwt}{{4.7}{43}{Continuous Wavelet Transformation}{equation.212}{}}
\newlabel{eq:orth_general}{{4.10}{43}{}{equation.217}{}}
\newlabel{eq:orth_real}{{4.11}{43}{}{equation.218}{}}
\newlabel{th:cwt_orth_rela}{{4.5}{43}{}{equation.218}{}}
\citation{daubechies1992ten}
\newlabel{eq:icwt}{{4.13}{44}{Inverse continuous wavelet transform}{equation.221}{}}
\newlabel{eq:icwt_real}{{4.14}{44}{Inverse continuous wavelet transform}{equation.222}{}}
\newlabel{def:icwt}{{4.6}{44}{Inverse continuous wavelet transform}{equation.222}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Discrete Wavelet Transformation}{44}{section.223}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Frames and orthogonal wavelet basis}{45}{subsection.224}\protected@file@percent }
\newlabel{eq:discr_wavelet}{{4.16}{45}{Frames and orthogonal wavelet basis}{equation.226}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}The scaling function}{46}{subsection.233}\protected@file@percent }
\newlabel{eq:scal_func}{{4.23}{46}{The scaling function}{equation.234}{}}
\newlabel{eq:approx_coef}{{4.25}{47}{The scaling function}{equation.236}{}}
\newlabel{eq:contin_approx}{{4.26}{47}{The scaling function}{equation.237}{}}
\newlabel{eq:detail_expansion}{{4.27}{47}{The scaling function}{equation.238}{}}
\newlabel{eq:detail_scale}{{4.28}{47}{The scaling function}{equation.239}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}The scaling equation}{48}{subsection.242}\protected@file@percent }
\newlabel{eq:scal_eq}{{4.31}{48}{The scaling equation}{equation.243}{}}
\newlabel{eq:scaling_coeff_constr}{{4.32}{48}{The scaling equation}{equation.244}{}}
\newlabel{eq:wavelet_eq}{{4.34}{48}{The scaling equation}{equation.246}{}}
\newlabel{eq:wavelet_eq_reorder}{{4.35}{48}{The scaling equation}{equation.247}{}}
\newlabel{eq:scal_multires}{{4.37}{49}{The scaling equation}{equation.249}{}}
\newlabel{eq:wavelet_multires}{{4.38}{49}{The scaling equation}{equation.250}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}The fast wavelet transform}{49}{subsection.251}\protected@file@percent }
\newlabel{eq:detail_coef_recur}{{4.40}{49}{The fast wavelet transform}{equation.253}{}}
\newlabel{eq:wavelet_coef_recur}{{4.41}{49}{The fast wavelet transform}{equation.254}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Schematic of DWT filtering}}{50}{figure.caption.255}\protected@file@percent }
\newlabel{fig:dwt_scheme}{{4.2}{50}{Schematic of DWT filtering}{figure.caption.255}{}}
\citation{strang1996wavelets}
\citation{donoho1994ideal}
\citation{donoho1995noising}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Noise estimation and Denoising}{51}{subsection.258}\protected@file@percent }
\newlabel{eq:threshold_esti}{{4.44}{52}{Noise estimation and Denoising}{equation.259}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces The VISUSHRINK Algorithm\relax }}{52}{algorithm.270}\protected@file@percent }
\newlabel{algo:visushrink}{{2}{52}{The VISUSHRINK Algorithm\relax }{algorithm.270}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Datasets, Training and Evaluation}{53}{chapter.271}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}BOLD inversion teacher forcing}{53}{section.272}\protected@file@percent }
\newlabel{eq:bold_inv_naive}{{5.2}{53}{BOLD inversion teacher forcing}{equation.274}{}}
\newlabel{eq:obs_eq_simple}{{5.9}{54}{BOLD inversion teacher forcing}{equation.281}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Boundary issues and minimum noise level}{55}{subsection.283}\protected@file@percent }
\newlabel{fig:high_freq_issues}{{5.1a}{56}{Deconvolution of a Lorenz63 time series with no noise component. Deconvolution returns high frequency artifacts both at the boundaries and throughout the signal.\relax }{figure.caption.284}{}}
\newlabel{sub@fig:high_freq_issues}{{a}{56}{Deconvolution of a Lorenz63 time series with no noise component. Deconvolution returns high frequency artifacts both at the boundaries and throughout the signal.\relax }{figure.caption.284}{}}
\newlabel{fig:min_conv_noise}{{5.1b}{56}{Deconvolution of a Lorenz63 time series with no noise component, but minimum noise level set. High frequency artifacts are much smaller and only appear close to the boundaries within the $\frac {1}{4}\text {length}(hrf)$ cutoffs.\relax }{figure.caption.284}{}}
\newlabel{sub@fig:min_conv_noise}{{b}{56}{Deconvolution of a Lorenz63 time series with no noise component, but minimum noise level set. High frequency artifacts are much smaller and only appear close to the boundaries within the $\frac {1}{4}\text {length}(hrf)$ cutoffs.\relax }{figure.caption.284}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Deconvolution issues exemplified by a Lorenz63 time series}}{56}{figure.caption.284}\protected@file@percent }
\newlabel{fig:three graphs}{{5.1}{56}{Deconvolution issues exemplified by a Lorenz63 time series}{figure.caption.284}{}}
\citation{mikhaeil2022difficulty}
\citation{koppe2019identifying}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces The full deconvolution Algorithm\relax }}{57}{algorithm.292}\protected@file@percent }
\newlabel{algo:deconv_full}{{3}{57}{The full deconvolution Algorithm\relax }{algorithm.292}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Evaluation metrics}{57}{section.293}\protected@file@percent }
\newlabel{sec:eval_metrics}{{5.2}{57}{Evaluation metrics}{section.293}{}}
\citation{brenner2022tractable}
\citation{hershey2007approximating}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}State Space Distance $D_{stsp}$}{58}{subsection.294}\protected@file@percent }
\newlabel{sec:d_stsp}{{5.2.1}{58}{State Space Distance \texorpdfstring {$D_{stsp}$}{Lg}}{subsection.294}{}}
\newlabel{eq:rel_freq}{{5.11}{58}{State Space Distance \texorpdfstring {$D_{stsp}$}{Lg}}{equation.295}{}}
\citation{mikhaeil2022difficulty}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Power Spectrum Error $D_{PSE}$}{59}{subsection.299}\protected@file@percent }
\newlabel{sec:d_pse}{{5.2.2}{59}{Power Spectrum Error \texorpdfstring {$D_{PSE}$}{Lg}}{subsection.299}{}}
\newlabel{eq:hellinger_dist}{{5.17}{59}{Power Spectrum Error \texorpdfstring {$D_{PSE}$}{Lg}}{equation.302}{}}
\newlabel{eq:d_pse}{{5.18}{59}{Power Spectrum Error \texorpdfstring {$D_{PSE}$}{Lg}}{equation.303}{}}
\citation{babayan2019mind}
\citation{lorenz1963deterministic}
\citation{babayan2019mind}
\citation{babayan2019mind}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Datasets}{60}{section.304}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Lorenz63 System}{60}{subsection.305}\protected@file@percent }
\citation{koppe2019identifying}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Simulated data of the Lorenz System}}{61}{figure.caption.309}\protected@file@percent }
\newlabel{fig:lorenz_system}{{5.2}{61}{Simulated data of the Lorenz System}{figure.caption.309}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}LEMON Dataset}{61}{subsection.310}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Example trajectories of fMRI time series}}{62}{figure.caption.311}\protected@file@percent }
\newlabel{fig:fmri_example}{{5.3}{62}{Example trajectories of fMRI time series}{figure.caption.311}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Experimental Setup: Benchmarking on Lorenz63 data}{62}{section.312}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Benchmarking on data without noise}{62}{subsection.313}\protected@file@percent }
\newlabel{sec:noiseless_lorenz}{{5.4.1}{62}{Benchmarking on data without noise}{subsection.313}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Convolution of the reconstructed Lorenz attractor with different $hrf$-functions}}{63}{figure.caption.319}\protected@file@percent }
\newlabel{fig:lorenz_conv}{{5.4}{63}{Convolution of the reconstructed Lorenz attractor with different $hrf$-functions}{figure.caption.319}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Benchmarking on data with noise}{63}{subsection.320}\protected@file@percent }
\newlabel{sec:noisy_lorenz}{{5.4.2}{63}{Benchmarking on data with noise}{subsection.320}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Example of noisy convoluted Lorenz attractor}}{64}{figure.caption.326}\protected@file@percent }
\newlabel{fig:lorenz_conv_noise}{{5.5}{64}{Example of noisy convoluted Lorenz attractor}{figure.caption.326}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Prediction Error on test set differing strongly between participants}}{65}{figure.caption.328}\protected@file@percent }
\newlabel{fig:high_var_patient_pe_comparison}{{5.6}{65}{Prediction Error on test set differing strongly between participants}{figure.caption.328}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Experimental Setup: Filtering LEMON datasets}{65}{section.327}\protected@file@percent }
\newlabel{sec:filter_lemon}{{5.5}{65}{Experimental Setup: Filtering LEMON datasets}{section.327}{}}
\citation{brenner2022tractable}
\citation{brenner2022tractable}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Simple moving average and variance of different participants}}{66}{figure.caption.333}\protected@file@percent }
\newlabel{fig:sma_smv_example}{{5.7}{66}{Simple moving average and variance of different participants}{figure.caption.333}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Correlation between time and moving variance}}{67}{figure.caption.334}\protected@file@percent }
\newlabel{fig:smv_cor_histo}{{5.8}{67}{Correlation between time and moving variance}{figure.caption.334}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Evaluation Setup: Choosing hyperparameters for $D_{stsp}$ and $D_{PSE}$}{67}{section.335}\protected@file@percent }
\newlabel{sec:eval_metrics_param}{{5.6}{67}{Evaluation Setup: Choosing hyperparameters for \texorpdfstring {$D_{stsp}$}{ Lg} and \texorpdfstring {$D_{PSE}$}{Lg}}{section.335}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces $D_{stsp}$ dependence on scaling parameter}}{68}{figure.caption.336}\protected@file@percent }
\newlabel{fig:dstsp_comp}{{5.9}{68}{$D_{stsp}$ dependence on scaling parameter}{figure.caption.336}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Grid search on LEMON dataset}{68}{section.338}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Normalized and smoothed power spectra for different $\sigma $}}{69}{figure.caption.337}\protected@file@percent }
\newlabel{fig:power_spectrum_sigma_comp}{{5.10}{69}{Normalized and smoothed power spectra for different $\sigma $}{figure.caption.337}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results}{70}{chapter.339}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Results on the benchmark systems}{70}{section.340}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Noiseless data}{70}{subsection.341}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Comparison of $D_{stsp}$ noiseless}}{71}{figure.caption.342}\protected@file@percent }
\newlabel{fig:Comparison_D_stsp_noiseless}{{6.1}{71}{Comparison of $D_{stsp}$ noiseless}{figure.caption.342}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Comparison of $D_{PSE}$ noiseless}}{71}{figure.caption.343}\protected@file@percent }
\newlabel{fig:Comparison_D_PSE_noiseless}{{6.2}{71}{Comparison of $D_{PSE}$ noiseless}{figure.caption.343}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Quantitative comparison between linear and convolutional observation model\relax }}{72}{table.caption.344}\protected@file@percent }
\newlabel{tab:noiseless_comp_table}{{6.1}{72}{Quantitative comparison between linear and convolutional observation model\relax }{table.caption.344}{}}
\newlabel{fig:traj_gen_3d_125}{{6.3a}{72}{At epoch 125\relax }{figure.caption.345}{}}
\newlabel{sub@fig:traj_gen_3d_125}{{a}{72}{At epoch 125\relax }{figure.caption.345}{}}
\newlabel{fig:traj_gen_3d_150}{{6.3b}{72}{At epoch 150\relax }{figure.caption.345}{}}
\newlabel{sub@fig:traj_gen_3d_150}{{b}{72}{At epoch 150\relax }{figure.caption.345}{}}
\newlabel{fig:traj_gen_3d_175}{{6.3c}{72}{At epoch 175\relax }{figure.caption.345}{}}
\newlabel{sub@fig:traj_gen_3d_175}{{c}{72}{At epoch 175\relax }{figure.caption.345}{}}
\newlabel{fig:traj_gen_3d_200}{{6.3d}{72}{At epoch 200\relax }{figure.caption.345}{}}
\newlabel{sub@fig:traj_gen_3d_200}{{d}{72}{At epoch 200\relax }{figure.caption.345}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Training instability in the presence of strong convolution}}{72}{figure.caption.345}\protected@file@percent }
\newlabel{fig:traj_gen_3d}{{6.3}{72}{Training instability in the presence of strong convolution}{figure.caption.345}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Noisy data}{72}{subsection.346}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Comparison of performance metrics on dataset convoluted with $hrf_{0.5}$ and added white noise}}{73}{figure.caption.347}\protected@file@percent }
\newlabel{fig:Comparison_hrf_0_5_noisy}{{6.4}{73}{Comparison of performance metrics on dataset convoluted with $hrf_{0.5}$ and added white noise}{figure.caption.347}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Quantitative comparison between linear and convolutional observation model on noisy data\relax }}{73}{table.caption.348}\protected@file@percent }
\newlabel{tab:noisy_comp_table}{{6.2}{73}{Quantitative comparison between linear and convolutional observation model on noisy data\relax }{table.caption.348}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Grid search on LEMON dataset}{74}{section.349}\protected@file@percent }
\newlabel{sec:results_gridsearch}{{6.2}{74}{Grid search on LEMON dataset}{section.349}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Results of the parameter search for $\ell $ values. Model runs where excluded if the 1-step PE $>$ 1\relax }}{74}{table.caption.350}\protected@file@percent }
\newlabel{tab:gridsearch_ldim_table}{{6.3}{74}{Results of the parameter search for $\ell $ values. Model runs where excluded if the 1-step PE $>$ 1\relax }{table.caption.350}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Grid search over latent dimension $ \ell $}}{75}{figure.caption.351}\protected@file@percent }
\newlabel{fig:grid_seach_latent_dimension}{{6.5}{75}{Grid search over latent dimension $ \ell $}{figure.caption.351}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Grid search over teacher forcing $\alpha $}}{76}{figure.caption.353}\protected@file@percent }
\newlabel{fig:grid_search_tf_alpha}{{6.6}{76}{Grid search over teacher forcing $\alpha $}{figure.caption.353}{}}
\citation{mcdonald2014handbook}
\citation{2020SciPy-NMeth}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Results of the parameter search for $\alpha $ values. Model runs where excluded if the 1-step PE $>$ 1\relax }}{77}{table.caption.352}\protected@file@percent }
\newlabel{tab:gridsearch_alpha_table}{{6.4}{77}{Results of the parameter search for $\alpha $ values. Model runs where excluded if the 1-step PE $>$ 1\relax }{table.caption.352}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Analysis of the results on the LEMON dataset}{77}{section.354}\protected@file@percent }
\newlabel{sec:results_main_run}{{6.3}{77}{Analysis of the results on the LEMON dataset}{section.354}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Quantitative Results of the PLRNN with BOLD observation model on the LEMON dataset. Model runs where excluded if the 1-step PE $>$ 1. $N_{converged}=1007$\relax }}{77}{table.caption.355}\protected@file@percent }
\newlabel{tab:main_run_results}{{6.5}{77}{Quantitative Results of the PLRNN with BOLD observation model on the LEMON dataset. Model runs where excluded if the 1-step PE $>$ 1. $N_{converged}=1007$\relax }{table.caption.355}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Distributions of metrics on train and test data}}{78}{figure.caption.356}\protected@file@percent }
\newlabel{fig:histograms_metrics_main_run}{{6.7}{78}{Distributions of metrics on train and test data}{figure.caption.356}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Wilcoxon signed-rank test applied to the distribution of metrics on training and test set. Model runs where excluded if the 1-step PE $>$ 1. $N_{converged}=1007$\relax }}{78}{table.caption.357}\protected@file@percent }
\newlabel{tab:wilcoxon_test_metrics_train_test}{{6.6}{78}{Wilcoxon signed-rank test applied to the distribution of metrics on training and test set. Model runs where excluded if the 1-step PE $>$ 1. $N_{converged}=1007$\relax }{table.caption.357}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Exemplary DS reconstruction in a sample subject}}{79}{figure.caption.358}\protected@file@percent }
\newlabel{fig:ds_recontruction_example}{{6.8}{79}{Exemplary DS reconstruction in a sample subject}{figure.caption.358}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Trajectories projected down to the first 3 principal components}}{80}{figure.caption.359}\protected@file@percent }
\newlabel{fig:pca_traj_reconstruction}{{6.9}{80}{Trajectories projected down to the first 3 principal components}{figure.caption.359}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Comparison of trajectories with different $D_{stsp}$ and $D_{PSE}$ combinations}}{81}{figure.caption.360}\protected@file@percent }
\newlabel{fig:metric_trajectory_comparison}{{6.10}{81}{Comparison of trajectories with different $D_{stsp}$ and $D_{PSE}$ combinations}{figure.caption.360}{}}
\citation{vogt2022lyapunov}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Further investigation of dynamical features}{82}{subsection.361}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Maximum Lyapunov exponents estimated over all model runs}}{82}{figure.caption.362}\protected@file@percent }
\newlabel{fig:max_le_of_patients}{{6.11}{82}{Maximum Lyapunov exponents estimated over all model runs}{figure.caption.362}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Comparison between a model with positive and a model with negative $\lambda _{max}$}}{82}{figure.caption.363}\protected@file@percent }
\newlabel{fig:pos_neg_le_comparison}{{6.12}{82}{Comparison between a model with positive and a model with negative $\lambda _{max}$}{figure.caption.363}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Model switching to harmonic oscillatory behavior in the time limit}}{83}{figure.caption.364}\protected@file@percent }
\newlabel{fig:oscillator_neg_le}{{6.13}{83}{Model switching to harmonic oscillatory behavior in the time limit}{figure.caption.364}{}}
\newlabel{eq:spec_norm_def}{{6.1}{83}{Further investigation of dynamical features}{equation.365}{}}
\citation{mcdonald2014handbook}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Spectral norm of W matrices for each participant estimated over all model runs}}{84}{figure.caption.366}\protected@file@percent }
\newlabel{fig:w_matrix_mean_std}{{6.14}{84}{Spectral norm of W matrices for each participant estimated over all model runs}{figure.caption.366}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Investigating Correlations}{84}{section.367}\protected@file@percent }
\newlabel{eq:cor_coef_def}{{6.2}{84}{Investigating Correlations}{equation.368}{}}
\citation{ikeda2017comprehensive}
\citation{dubois2018resting}
\citation{spielberger1970manual}
\citation{hamilton1960rating}
\citation{costa1989neo}
\newlabel{fig:corr_matrix_max_le}{{6.15a}{85}{Correlation matrix of maximum Lypunov exponent over all converged runs}{figure.caption.369}{}}
\newlabel{sub@fig:corr_matrix_max_le}{{a}{85}{Correlation matrix of maximum Lypunov exponent over all converged runs}{figure.caption.369}{}}
\newlabel{fig:corr_matrix_max_le_filtered}{{6.15b}{85}{Correlation matrix of maximum Lypunov exponent over filtered model runs}{figure.caption.369}{}}
\newlabel{sub@fig:corr_matrix_max_le_filtered}{{b}{85}{Correlation matrix of maximum Lypunov exponent over filtered model runs}{figure.caption.369}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Correlation matrices of the maximum Lypunov exponents}}{85}{figure.caption.369}\protected@file@percent }
\newlabel{fig:max_le_cor_matrices}{{6.15}{85}{Correlation matrices of the maximum Lypunov exponents}{figure.caption.369}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Correlation matrices of W matrices spectral norms over all model runs}}{86}{figure.caption.370}\protected@file@percent }
\newlabel{fig:corr_matrix_w_spec_norm}{{6.16}{86}{Correlation matrices of W matrices spectral norms over all model runs}{figure.caption.370}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces Correlation matrices of W matrices spectral norms over filtered model runs}}{86}{figure.caption.371}\protected@file@percent }
\newlabel{fig:corr_matrix_spec_norms_filtered}{{6.17}{86}{Correlation matrices of W matrices spectral norms over filtered model runs}{figure.caption.371}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.7}{\ignorespaces Correlation between personality measures and mean maximum Lyapunov exponents. Mean $\lambda _{max}$ is calculated from all converged models. The results are statistically significant at a p-value$<0.05$. No correction for multiple testing was performed.\relax }}{87}{table.caption.372}\protected@file@percent }
\newlabel{tab:mean_le_personality_cor_table}{{6.7}{87}{Correlation between personality measures and mean maximum Lyapunov exponents. Mean $\lambda _{max}$ is calculated from all converged models. The results are statistically significant at a p-value$<0.05$. No correction for multiple testing was performed.\relax }{table.caption.372}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.8}{\ignorespaces Correlation between personality measures and mean maximum Lyapunov exponents. Mean $\lambda _{max}$ is only calculated from filtered models. The results are statistically significant at a p-value$<0.05$. No correction for multiple testing was performed.\relax }}{87}{table.caption.373}\protected@file@percent }
\newlabel{tab:mean_le_filtered_personality_cor_table}{{6.8}{87}{Correlation between personality measures and mean maximum Lyapunov exponents. Mean $\lambda _{max}$ is only calculated from filtered models. The results are statistically significant at a p-value$<0.05$. No correction for multiple testing was performed.\relax }{table.caption.373}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Discussion and Outlook}{88}{chapter.374}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Discussion}{88}{section.375}\protected@file@percent }
\citation{cakan2021}
\citation{mikhaeil2022difficulty}
\citation{gonzalez2023manifold}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Outlook}{90}{section.376}\protected@file@percent }
\citation{brenner2022multimodal}
\@writefile{toc}{\vspace  {2em}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix}{92}{appendix.377}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Proofs of Wavelet theorems}{92}{section.378}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Proof for orthogonality relation for the wavelet transform, theorem \ref  {th:cwt_orth_rela}}{92}{subsection.379}\protected@file@percent }
\newlabel{proof:cwt_orth_rela}{{A.1.1}{92}{Proof for orthogonality relation for the wavelet transform, theorem \ref {th:cwt_orth_rela}}{subsection.379}{}}
\newlabel{eq:wt_orth_proof}{{A.5}{92}{Proof for orthogonality relation for the wavelet transform, theorem \ref {th:cwt_orth_rela}}{equation.384}{}}
\newlabel{eq:wt_orth_proof_simple}{{A.6}{92}{Proof for orthogonality relation for the wavelet transform, theorem \ref {th:cwt_orth_rela}}{equation.385}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.2}Proof of Inverse continuous wavelet transform, definition \ref  {def:icwt}}{93}{subsection.387}\protected@file@percent }
\newlabel{proof:icwt}{{A.1.2}{93}{Proof of Inverse continuous wavelet transform, definition \ref {def:icwt}}{subsection.387}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Hyperparameter Settings}{93}{section.392}\protected@file@percent }
\@writefile{toc}{\vspace  {2em}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Hyperparameter settings for Lorenz benchmark runs.\relax }}{94}{table.caption.393}\protected@file@percent }
\newlabel{tab:args lorenz runs}{{A.1}{94}{Hyperparameter settings for Lorenz benchmark runs.\relax }{table.caption.393}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Hyperparameter settings for training on the LEMON dataset.\relax }}{95}{table.caption.394}\protected@file@percent }
\newlabel{tab:args lemon runs}{{A.2}{95}{Hyperparameter settings for training on the LEMON dataset.\relax }{table.caption.394}{}}
\bibstyle{unsrtnat}
\bibdata{Bibliography}
\bibcite{dilao2023dynamical}{{1}{2023}{{Dil{\~a}o}}{{}}}
\bibcite{strogatz2018nonlinear}{{2}{2018}{{Strogatz}}{{}}}
\bibcite{benettin1980lyapunov}{{3}{1980}{{Benettin et~al.}}{{Benettin, Galgani, Giorgilli, and Strelcyn}}}
\bibcite{vogt2022lyapunov}{{4}{2022}{{Vogt et~al.}}{{Vogt, Puelma~Touzel, Shlizerman, and Lajoie}}}
\bibcite{chung2021turing}{{5}{2021}{{Chung and Siegelmann}}{{}}}
\bibcite{schafer2006recurrent}{{6}{2006}{{Sch{\"a}fer and Zimmermann}}{{}}}
\bibcite{blum1988training}{{7}{1988}{{Blum and Rivest}}{{}}}
\bibcite{bengio1993problem}{{8}{1993}{{Bengio et~al.}}{{Bengio, Frasconi, and Simard}}}
\bibcite{bengio1994learning}{{9}{1994}{{Bengio et~al.}}{{Bengio, Simard, and Frasconi}}}
\bibcite{pascanu2013difficulty}{{10}{2013}{{Pascanu et~al.}}{{Pascanu, Mikolov, and Bengio}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{96}{dummy.395}\protected@file@percent }
\newlabel{Bibliography}{{3}{96}{Hyperparameter Settings}{dummy.395}{}}
\bibcite{hochreiter1997long}{{11}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{le2015simple}{{12}{2015}{{Le et~al.}}{{Le, Jaitly, and Hinton}}}
\bibcite{talathi2015improving}{{13}{2015}{{Talathi and Vartak}}{{}}}
\bibcite{williams1989learning}{{14}{1989}{{Williams and Zipser}}{{}}}
\bibcite{jordan1986attractor}{{15}{1986}{{Jordan}}{{}}}
\bibcite{pineda1988dynamics}{{16}{1988}{{Pineda}}{{}}}
\bibcite{NEURIPS2022_495e55f3}{{17}{2022{}}{{Mikhaeil et~al.}}{{Mikhaeil, Monfared, and Durstewitz}}}
\bibcite{doya1992bifurcations}{{18}{1992}{{Doya et~al.}}{{}}}
\bibcite{durstewitz2017state}{{19}{2017}{{Durstewitz}}{{}}}
\bibcite{koppe2019identifying}{{20}{2019}{{Koppe et~al.}}{{Koppe, Toutounji, Kirsch, Lis, and Durstewitz}}}
\bibcite{schmidt2019identifying}{{21}{2019}{{Schmidt et~al.}}{{Schmidt, Koppe, Monfared, Beutelspacher, and Durstewitz}}}
\bibcite{brenner2022tractable}{{22}{2022{}}{{Brenner et~al.}}{{Brenner, Hess, Mikhaeil, Bereska, Monfared, Kuo, and Durstewitz}}}
\bibcite{mikhaeil2022difficulty}{{23}{2022{}}{{Mikhaeil et~al.}}{{Mikhaeil, Monfared, and Durstewitz}}}
\bibcite{hess2023generalized}{{24}{2023}{{Hess et~al.}}{{Hess, Monfared, Brenner, and Durstewitz}}}
\bibcite{fox2012coupling}{{25}{2012}{{Fox}}{{}}}
\bibcite{penny2011statistical}{{26}{2011}{{Penny et~al.}}{{Penny, Friston, Ashburner, Kiebel, and Nichols}}}
\bibcite{serov2017fourier}{{27}{2017}{{Serov et~al.}}{{}}}
\bibcite{smith1997scientist}{{28}{1997}{{Smith et~al.}}{{}}}
\bibcite{puthusserypady2021applied}{{29}{2021}{{Puthusserypady}}{{}}}
\bibcite{FFTW.jl-2005}{{30}{2005}{{Frigo and Johnson}}{{}}}
\bibcite{jones1970problem}{{31}{1970}{{Jones and Misell}}{{}}}
\bibcite{wiener1964extrapolation}{{32}{1964}{{Wiener}}{{}}}
\bibcite{sauer-notes}{{33}{2012}{{schafer2006recurrent}}{{}}}
\bibcite{mallat2008wavelet}{{34}{2008}{{Mallat}}{{}}}
\bibcite{shah2022wavelet}{{35}{2022}{{Shah and Tantary}}{{}}}
\bibcite{daubechies1992ten}{{36}{1992}{{Daubechies}}{{}}}
\bibcite{strang1996wavelets}{{37}{1996}{{Strang and Nguyen}}{{}}}
\bibcite{donoho1994ideal}{{38}{1994}{{Donoho and Johnstone}}{{}}}
\bibcite{donoho1995noising}{{39}{1995}{{Donoho}}{{}}}
\bibcite{hershey2007approximating}{{40}{2007}{{Hershey and Olsen}}{{}}}
\bibcite{babayan2019mind}{{41}{2019}{{Babayan et~al.}}{{Babayan, Erbey, Kumral, Reinelt, Reiter, R{\"o}bbig, Schaare, Uhlig, Anwander, Bazin, et~al.}}}
\bibcite{lorenz1963deterministic}{{42}{1963}{{Lorenz}}{{}}}
\bibcite{mcdonald2014handbook}{{43}{2014}{{McDonald}}{{}}}
\bibcite{2020SciPy-NMeth}{{44}{2020}{{Virtanen et~al.}}{{Virtanen, Gommers, Oliphant, Haberland, Reddy, Cournapeau, Burovski, Peterson, Weckesser, Bright, {van der Walt}, Brett, Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng, Moore, {VanderPlas}, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris, Archibald, Ribeiro, Pedregosa, {van Mulbregt}, and {SciPy 1.0 Contributors}}}}
\bibcite{ikeda2017comprehensive}{{45}{2017}{{Ikeda et~al.}}{{Ikeda, Takeuchi, Taki, Nouchi, Yokoyama, Kotozaki, Nakagawa, Sekiguchi, Iizuka, Yamamoto, et~al.}}}
\bibcite{dubois2018resting}{{46}{2018}{{Dubois et~al.}}{{Dubois, Galdi, Han, Paul, and Adolphs}}}
\bibcite{spielberger1970manual}{{47}{1970}{{Spielberger}}{{}}}
\bibcite{hamilton1960rating}{{48}{1960}{{Hamilton}}{{}}}
\bibcite{costa1989neo}{{49}{1989}{{Costa and McCrae}}{{}}}
\bibcite{cakan2021}{{50}{2021}{{Cakan et~al.}}{{Cakan, Jajcay, and Obermayer}}}
\bibcite{gonzalez2023manifold}{{51}{2023}{{Gonzalez-Castillo et~al.}}{{Gonzalez-Castillo, Fernandez, Lam, Handwerker, Pereira, and Bandettini}}}
\bibcite{brenner2022multimodal}{{52}{2022{}}{{Brenner et~al.}}{{Brenner, Koppe, and Durstewitz}}}
